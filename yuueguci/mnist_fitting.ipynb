{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_fitting.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/yuu-eguci/flower-stuff-lab/blob/main/yuueguci/sample.ipynb",
      "authorship_tag": "ABX9TyOyJbnZc8NDESYyMgUcpKq9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuu-eguci/flower-stuff-lab/blob/main/yuueguci/mnist_fitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBgFLQ6hEnk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f48e3c7-a1e8-494d-84a1-f5d60f768710"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jun 28 23:15:23 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r36MTtu9KBrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "036286e4-5664-4350-9181-63f38f8df72f"
      },
      "source": [
        "# Google Drive をマウントします。\n",
        "# NOTE: 左のトコをポチポチやってマウントすることも出来ますが(というかそのほうがラク)\n",
        "# マウントすることを明示するほうが好みなのでしています。\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMO_qgOjb3wu",
        "outputId": "66380c1d-a571-46ee-cb04-b2a78531cc95"
      },
      "source": [
        "# Keras のバージョンが結構重要な感あります。確認しておきます。\n",
        "!pip list | grep -e Keras -e tensorflow -e h5py -e Pillow -e opencv-python"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "h5py                          3.1.0              \n",
            "Keras                         2.4.3              \n",
            "Keras-Preprocessing           1.1.2              \n",
            "opencv-python                 4.1.2.30           \n",
            "Pillow                        7.1.2              \n",
            "tensorflow                    2.5.0              \n",
            "tensorflow-datasets           4.0.1              \n",
            "tensorflow-estimator          2.5.0              \n",
            "tensorflow-gcs-config         2.5.0              \n",
            "tensorflow-hub                0.12.0             \n",
            "tensorflow-metadata           1.0.0              \n",
            "tensorflow-probability        0.12.1             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTmUDFLT3o4X"
      },
      "source": [
        "\"\"\"mnist_fitting main script です。\n",
        "NOTE: 全コードを同じ code block に詰め込んでいます。\n",
        "      Notebook の標準的な使い方と違うと思いますが、慣れるまでは普段やってる感じで使ってみます。\n",
        "\"\"\"\n",
        "\n",
        "# NOTE: Colaboratory で module imported but unused とか出す方法あるのかな?\n",
        "# NOTE: この内容なら、 Colab では pip install 不要です。\n",
        "import os\n",
        "import time\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.datasets import mnist\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "\n",
        "# これはサンプルにはない。\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "\n",
        "class MNISTDataset():\n",
        "    \"\"\"MNIST dataset をクラスとして定義しています。\n",
        "    NOTE: image_shape とか、グローバルスコープで定数で定義するのイヤやん?\n",
        "    NOTE: インスタンス変数が変わることって無いと思うから、 static class にすべきじゃない?\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        # MNIST は 28x28x1(グレースケール)のデータセットです。\n",
        "        self.image_shape = (28, 28, 1)\n",
        "        # クラス数は当然 0~9 の10個です。\n",
        "        self.num_classes = 10\n",
        "\n",
        "    def get_batch(self):\n",
        "\n",
        "        # NOTE: このメソッド名はサンプルママです。\n",
        "        #       強化学習の方式をオンライン学習といい、既存データセットから学習することをバッチ学習という。\n",
        "        #       「バッチ学習」用のデータだから batch なのかな。\n",
        "\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        # preprocess により 0~1 の値に変換します。\n",
        "        # label_data(正解データ)は one-hot ベクトルへ変換します。\n",
        "        # NOTE: one-hot ベクトルは、ラベルの数(10)と等しい数のベクトルです。\n",
        "        # NOTE: ここはよくわかっていない。\n",
        "        x_train, x_test = [self.preprocess(d) for d in [x_train, x_test]]\n",
        "        y_train, y_test = [self.preprocess(d, label_data=True) for d in\n",
        "                           [y_train, y_test]]\n",
        "\n",
        "        return x_train, y_train, x_test, y_test\n",
        "\n",
        "    def preprocess_for_image(self, data):\n",
        "\n",
        "        data = data.astype('float32')\n",
        "        # 画像データを 0~1 の値に変換しています。\n",
        "        data /= 255\n",
        "        shape = (data.shape[0],) + self.image_shape  # add dataset length\n",
        "        data = data.reshape(shape)\n",
        "        return data\n",
        "\n",
        "    def preprocess_for_label(self, data):\n",
        "\n",
        "        # ラベルデータを one-hot ベクトルに変換しています。\n",
        "        # XXX: one-hot を理解していません。\n",
        "        return to_categorical(data, self.num_classes)\n",
        "\n",
        "\n",
        "def lenet(input_shape, num_classes):\n",
        "    \"\"\"LeNet を定義します。\n",
        "    NOTE: LeNet は最初期の CNN です。\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # 畳み込み層を追加します。\n",
        "    # NOTE: 畳み込む領域5x5、フィルタ20枚、特徴マップのサイズは入力と同じ、\n",
        "    #       活性化関数 ReLU です。\n",
        "    # NOTE: input_shape を指定する必要があります。入力を受け取る最初の層なので。\n",
        "    # NOTE: 入力が28x28の画像なら、出力は28x28x20です。\n",
        "    model.add(Conv2D(\n",
        "        20,\n",
        "        kernel_size=5,\n",
        "        padding='same',\n",
        "        input_shape=input_shape,\n",
        "        activation='relu',\n",
        "    ))\n",
        "    # 畳み込み層にはプーリング層を追加。\n",
        "    # NOTE: 2x2の領域ごとに圧縮するので、上の Conv2D とあわせて\n",
        "    #       特徴マップは14x14x20になります。\n",
        "    model.add(MaxPooling2D(\n",
        "        pool_size=(2, 2)\n",
        "    ))\n",
        "\n",
        "    # 畳み込み層追加です。\n",
        "    # NOTE: 最初の Conv2D よりもフィルタが多いです。\n",
        "    #       深い層ほどフィルタを増やすことは、 CNN における一般的なテクです。\n",
        "    # NOTE: 入力は14x14x20の特徴マップです(↑でコメントした条件の場合)。\n",
        "    #       padding=same なので出力も 14x14x50 です。\n",
        "    model.add(Conv2D(\n",
        "        50,\n",
        "        kernel_size=5,\n",
        "        padding='same',\n",
        "        activation='relu')\n",
        "    )\n",
        "    # プーリング層追加です。\n",
        "    # NOTE: 更に小さく、7x7x50になります。\n",
        "    model.add(MaxPooling2D(\n",
        "        pool_size=(2, 2)\n",
        "    ))\n",
        "\n",
        "    # 特徴マップを全結合層と接続できるようにします。\n",
        "    # NOTE: 特徴マップ->ベクトルの変換らしい。\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # 全結合層です。サイズ500のベクトルを出力します。\n",
        "    model.add(Dense(\n",
        "        500,\n",
        "        activation='relu',\n",
        "    ))\n",
        "    # 分類クラスぶんのベクトルを出力します。\n",
        "    model.add(Dense(\n",
        "        num_classes,\n",
        "    ))\n",
        "\n",
        "    # 活性化関数 softmax により、値を 0~1 の確率値へ変換します。\n",
        "    # XXX: 全結合層がクラスぶんの値を出力するかと思ったが、そういうわけではなかったか。\n",
        "    # NOTE: 活性化関数は(多分)Dense の定義に含めることも可能。\n",
        "    #       (activation='softmax' のように)\n",
        "    #       でもこうして別途書くこともできる。\n",
        "    #       おそらく↑の XXX の疑問の答えはこれ。\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "class Trainer():\n",
        "    \"\"\"Fitting を行うクラスです。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, loss, optimizer):\n",
        "\n",
        "        self._target = model\n",
        "        self._target.compile(\n",
        "            loss=loss,\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'],\n",
        "        )\n",
        "        self.verbose = 1\n",
        "        self.log_dir = '/content/drive/MyDrive/TensorBoardLogs/mnist_fitting'\n",
        "        self.hdf5_file_name = '/content/drive/MyDrive/hdf5/mnist_fitting.hdf5'\n",
        "\n",
        "    def train(self, x_train, y_train, batch_size, epochs, validation_split):\n",
        "\n",
        "        # 前回の実行のログを削除しています。\n",
        "        if os.path.exists(self.log_dir):\n",
        "            import shutil\n",
        "            shutil.rmtree(self.log_dir)\n",
        "        os.mkdir(self.log_dir)\n",
        "\n",
        "        self._target.fit(\n",
        "            # Input data, numpy array\n",
        "            x_train,\n",
        "            # Target data, numpy array or tensorflow tensor\n",
        "            y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_split=validation_split,\n",
        "            callbacks=[\n",
        "                TensorBoard(log_dir=self.log_dir),\n",
        "                ModelCheckpoint(self.hdf5_file_name, save_best_only=True),\n",
        "            ],\n",
        "            verbose=self.verbose,\n",
        "        )\n",
        "\n",
        "\n",
        "# MNIST を読み込みます。\n",
        "mnist_dataset = MNISTDataset()\n",
        "\n",
        "# LeNet モデルを作成します。\n",
        "lenet_model = lenet(\n",
        "    mnist_dataset.image_shape,\n",
        "    mnist_dataset.num_classes,\n",
        ")\n",
        "\n",
        "# MNIST からデータセットを取得します。\n",
        "x_train, y_train, x_test, y_test = mnist_dataset.get_batch()\n",
        "\n",
        "# Trainer にデータセットを渡します。\n",
        "trainer = Trainer(\n",
        "    lenet_model,\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=Adam(),\n",
        ")\n",
        "\n",
        "# Fitting します。\n",
        "# NOTE: 毎回学習するのは非常にクソ面倒でいらっしゃいますので保存をしたいですね?\n",
        "#       Trainer.train でわかるように ModelCheckpoint によって保存しています。\n",
        "start = time.time()\n",
        "trainer.train(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=128,\n",
        "    epochs=12,\n",
        "    validation_split=0.2,\n",
        ")\n",
        "print(f'Duration until fitting done: {time.time() - start} sec')\n",
        "\n",
        "# 結果を閲覧します。\n",
        "score = lenet_model.evaluate(\n",
        "    x_test,\n",
        "    y_test,\n",
        "    verbose=0,\n",
        ")\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqlEfqsFdMGe"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/TensorBoardLogs/mnist_fitting"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}