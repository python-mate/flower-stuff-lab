{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sample.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1syHXpqnvDWHR4QREJJuO16uQQoXybUYq",
      "authorship_tag": "ABX9TyMQ/YPelmMTfySp6Fy4kH1W",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuu-eguci/flower-stuff-lab/blob/main/goodluckpenpen/sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ1a90xtqbvb"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZdLuQTAX5SQ",
        "outputId": "8161d416-64f4-4a44-8bbd-28494e673fbd"
      },
      "source": [
        "# Google Drive をマウントします。\n",
        "# NOTE: 左のトコをポチポチやってマウントすることも出来ますが、マウントすることを明示するほうが好みなのでしています。\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRLUdJXesE2Y"
      },
      "source": [
        "# 17flowers dataset を取得します。\n",
        "# NOTE: マイドライブに 17flowers.zip を置いてある必要があります。自分で用意してください。\n",
        "#       /content は一時領域なので、数時間ごとに消滅します。\n",
        "#       だからイチイチ、 unzip しています。\n",
        "!unzip \"/content/drive/MyDrive/datasets/17flowers.zip\" -d \"/content\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-rKPiPDsd3A"
      },
      "source": [
        "# 学習用画像が格納されているフォルダです。\n",
        "TRAIN_DIR = '/content/17flowers/train_images'\n",
        "# テスト用画像が格納されているフォルダです。\n",
        "TEST_DIR = '/content/17flowers/test_images'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryHE4jKyseFb"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Input\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr_zcXY0seKM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "72a637cc-bba7-4440-ae1d-1d6cb2719bc4"
      },
      "source": [
        "# VGG16 のデフォルトである 224x224 でインプットを定義します。\n",
        "# NOTE: <class 'keras.engine.keras_tensor.KerasTensor'>\n",
        "# NOTE: Tensor は\n",
        "#       > 線形的な量または線形的な幾何概念を一般化したもので、基底を選べば、多次元の配列として表現できるようなものである。\n",
        "#       > (Wikipedia より)\n",
        "#       です。(?)\n",
        "input_tensor = Input(shape=(224, 224, 3))\n",
        "\n",
        "# VGG16 をロードします。\n",
        "# が、今回はフル結合3層をつけずにロードしています。\n",
        "# NOTE: include_top=False がフル結合3層をつけないという指定です。\n",
        "#       VGG16 は畳み込み13層とフル結合3層の計16層から成ります。\n",
        "#       なので、いうなれば VGG13 になってるってこと。\n",
        "#       (これは理解しやすいからそう書いているだけで誤解なきよう。)\n",
        "# NOTE: <class 'keras.engine.functional.Functional'>\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
        "\n",
        "# 新たな層を追加しています。\n",
        "# この output っていうのが現在の最後の13層目のことです。\n",
        "# NOTE: <class 'keras.engine.keras_tensor.KerasTensor'>\n",
        "x = base_model.output\n",
        "\n",
        "# GlobalAveragePooling2D という層を足しています。(14層目)\n",
        "# NOTE: さっき VGG13 だったのが VGG14 になるってこと。\n",
        "#       (これは理解しやすいからそう書いているだけなので他所で言わないように。)\n",
        "# NOTE: Dense は時間がかかるが GlobalAveragePooling は高速だという話です。\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Dense という層を足しています。(15層目)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "\n",
        "# ここが自分の追加したい層。(16層目)\n",
        "y = Dense(17, activation='softmax')(x)\n",
        "\n",
        "# 完成したこれが層のかたまり。\n",
        "# NOTE: x とか y とかって変数名を使っているのは、このモデルの構築手順は数式で表せる(らしい)からです。\n",
        "#       こんな感じに。こういうのって数式って言うの? 方程式じゃなくて?\n",
        "#       y = Dense(Dense(GlobalAveragePooling2D(x)))\n",
        "# NOTE: <class 'keras.engine.functional.Functional'>\n",
        "model = Model(inputs=base_model.input, outputs=y)\n",
        "\n",
        "# 構築した改造 VGG16 を閲覧します。\n",
        "# VGG1 6の構造に加え、最後に層が追加されている事がわかります。\n",
        "# NOTE: こういうのが増えてる\n",
        "#       dense_1 (Dense)              (None, 17)                17425\n",
        "#       たぶん17ってのが Dense(17... で追加した層だろう。\n",
        "model.summary()\n",
        "\n",
        "# VGG16 の全層の重みを固定しています。\n",
        "# VGG16 側の層の重みは学習時に変更されません。\n",
        "# base_model は最初に用意した13層のこと。\n",
        "# これはもう学習終わってんのだから(imagenet で)、 train する必要なしです。\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# モデルを作っただけだと線形の結果しか出ません。\n",
        "# y = a * b * c * x みたいなものだからです。\n",
        "# 係数を自分で変更させるように……\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "        # NOTE: サンプルコードでは lr になっていたが、\n",
        "        #       UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
        "        #       が出るので learning_rate へ変更しました。\n",
        "        learning_rate=0.0001,\n",
        "        momentum=0.9,\n",
        "    ),\n",
        "    # 右辺と左辺の差を小さくするためのもの。微分です。\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# Training に使う画像を生成する ImageDataGenerator を作ります。\n",
        "# NOTE: ImageDataGenerator は与えた画像をいじり、 training に使う画像パターンを増やします。\n",
        "#       https://keras.io/ja/preprocessing/image/\n",
        "# NOTE: <class 'keras.preprocessing.image.ImageDataGenerator'>\n",
        "image_data_generator_to_train = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=10,\n",
        ")\n",
        "\n",
        "# 予測? 類推? 推測?(用語がわからん)テストに使う画像を生成する ImageDataGenerator を作ります。\n",
        "# NOTE: どうして小さくしているのかというと、モデルは小数点で学習するからです。\n",
        "#       どういうこと?\n",
        "image_data_generator_to_test = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        ")\n",
        "\n",
        "# ImageDataGenerator へ画像を与えます。\n",
        "# NOTE: <class 'keras.preprocessing.image.DirectoryIterator'>\n",
        "directory_iterator_for_training = image_data_generator_to_train.flow_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "directory_iterator_for_test = image_data_generator_to_test.flow_from_directory(\n",
        "    TEST_DIR,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# ここで学習を行います。なので時間かかります。\n",
        "# fit は学習のメソッドです。\n",
        "# NOTE: サンプルコードでは Model.fit_generator になっていたが、\n",
        "#       UserWarning: `Model.fit_generator` is deprecated and\n",
        "#                    will be removed in a future version.\n",
        "#                    Please use `Model.fit`, which supports generators.\n",
        "#       が出るため model.fit に変更しました。\n",
        "# NOTE: <class 'keras.callbacks.History'>\n",
        "history = model.fit(\n",
        "    directory_iterator_for_training,\n",
        "    # NOTE: 1260 はトレーニング用の枚数です。 70*18=1260\n",
        "    steps_per_epoch=1260 // 16,\n",
        "    epochs=60,\n",
        "    verbose=1,\n",
        "    validation_data=directory_iterator_for_test,\n",
        "    # NOTE: 180 はテスト用の枚数です。 10*18=180\n",
        "    validation_steps=180 // 16,\n",
        ")\n",
        "print(type(history))\n",
        "\n",
        "# 保存したものを予測にしか使わないなら include_optimizer=False を設定しておくと、サイズが半分以下になるそうだ。\n",
        "# NOTE: 保存したファイルを利用するのは別ファイルです。\n",
        "model.save('17flowers.hdf5')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 17)                17425     \n",
            "=================================================================\n",
            "Total params: 15,257,425\n",
            "Trainable params: 15,257,425\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Found 1260 images belonging to 18 classes.\n",
            "Found 180 images belonging to 18 classes.\n",
            "Epoch 1/60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2da4531220fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirectory_iterator_for_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# NOTE: 180 はテスト用の枚数です。 10*18=180\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m180\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m )\n\u001b[1;32m    127\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 _r=1):\n\u001b[1;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  logits and labels must be broadcastable: logits_size=[16,17] labels_size=[16,18]\n\t [[node categorical_crossentropy/softmax_cross_entropy_with_logits (defined at /usr/local/lib/python3.7/dist-packages/keras/backend.py:4842) ]] [Op:__inference_train_function_1263]\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0TlommCseQT"
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, GlobalAveragePooling2D, Dense\n",
        "from keras.preprocessing import image\n",
        "import numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyuXOlp0seXA"
      },
      "source": [
        "CLASSES_FOR_17FLOWERS = [\n",
        "    'Tulip', 'Snowdrop', 'LilyValley', 'Bluebell', 'Crocus',\n",
        "    'Iris', 'Tigerlily', 'Daffodil', 'Fritillary', 'Sunflower',\n",
        "    'Daisy', 'ColtsFoot', 'Dandelion', 'Cowslip', 'Buttercup',\n",
        "    'Windflower', 'Pansy', 'Anthurium',\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7Ypr9bbseZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c48afb0c-bf56-4db5-ec55-f5d15e4038b3"
      },
      "source": [
        "# test_vgg16.py で行っているのと同じように、\n",
        "# 引数で与えられた画像を、予測へまわせる形式へ変換します。\n",
        "# <class 'PIL.Image.Image'>\n",
        "#       -> 3次元テンソル <class 'numpy.ndarray'>\n",
        "#       -> 4次元テンソル <class 'numpy.ndarray'>\n",
        "image_pil = image.load_img('/content/drive/MyDrive/keras-vgg16-lab/test-images/sunflower.jpg', target_size=(224, 224))\n",
        "image_array_3dim = image.img_to_array(image_pil)\n",
        "image_array_4dim = numpy.expand_dims(image_array_3dim, axis=0)\n",
        "\n",
        "# 学習時(fine_tuning_17flowers.py)において、 ImageDataGenerator の rescale で正規化したので同じ処理が必要。\n",
        "# XXX: ……らしいですよくわかんないです。\n",
        "# これを忘れると結果がおかしくなる。\n",
        "# XXX: ……らしいですよくわかんないです。\n",
        "# NOTE: どうおかしくなるのかわからなかったのでこれをやらないで試しました。\n",
        "#       やって sunflower.jpg 試した場合↓\n",
        "#           ('Cowslip', 0.14450616)\n",
        "#           ('Sunflower', 0.08813832)\n",
        "#           ('Tigerlily', 0.07421722)\n",
        "#       やらないで試した場合↓\n",
        "#           ('Cowslip', 0.9992175)\n",
        "#           ('Tigerlily', 0.0007754794)\n",
        "#           ('Buttercup', 6.109471e-06)\n",
        "image_array_4dim = image_array_4dim / 255.0\n",
        "\n",
        "# fine_tuning_17flowers.py で一番最初に作っているのと同じ、 VGG13(仮) + 自作3層 を用意します。\n",
        "# NOTE: Fine tuning 関連の記事ではよく Sequential が使われているけどまずこれでいく。\n",
        "input_tensor = Input(shape=(224, 224, 3))\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "y = Dense(17, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=y)\n",
        "\n",
        "# Fine tuning ではこのあと重み固定(layer.trainable = False)とかをしています。\n",
        "# 今回は学習を行う必要がありません。 Weight file はすでにあるので。\n",
        "model.load_weights('/content/17flowers.hdf5')\n",
        "\n",
        "# NOTE: predict 前に model.compile する必要はありますか?\n",
        "#       試しました。\n",
        "#       やって sunflower.jpg 試した場合↓\n",
        "#           ('Cowslip', 0.14450616)\n",
        "#           ('Sunflower', 0.08813832)\n",
        "#           ('Tigerlily', 0.07421722)\n",
        "#       やらないで試した場合。あ、変わらないじゃん。やらないでいいみたい。\n",
        "#           ('Cowslip', 0.14450616)\n",
        "#           ('Sunflower', 0.08813832)\n",
        "#           ('Tigerlily', 0.07421722)\n",
        "\n",
        "# 予測を行います。\n",
        "# NOTE: test_vgg16 では\n",
        "#       predictions = vgg16.predict(preprocess_input(image_array_4dim))\n",
        "#       top_5_predictions = decode_predictions(predictions, top=5)[0]\n",
        "#       でしたね。どういうことやねん。\n",
        "# NOTE: test_vgg16.py では preprocess_input を使っていたのにどうしてこっちでは使わないのだろう?\n",
        "#       と思ったので使って試しました。\n",
        "#       使わず sunflower.jpg 試した場合↓\n",
        "#           ('Cowslip', 0.14450616)\n",
        "#           ('Sunflower', 0.08813832)\n",
        "#           ('Tigerlily', 0.07421722)\n",
        "#       使って試した場合。 Sunflower が消えた。\n",
        "#           ('Iris', 0.16075435)\n",
        "#           ('Tulip', 0.15425675)\n",
        "#           ('Fritillary', 0.13122706)\n",
        "prediction = model.predict(image_array_4dim)[0]\n",
        "top_indices = prediction.argsort()[-5:][::-1]\n",
        "result = [(CLASSES_FOR_17FLOWERS[i], prediction[i]) for i in top_indices]\n",
        "for x in result:\n",
        "    print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Cowslip', 0.27655578)\n",
            "('Sunflower', 0.09722084)\n",
            "('Tigerlily', 0.09410678)\n",
            "('Pansy', 0.068630874)\n",
            "('Snowdrop', 0.062643126)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XOXfkjXsebV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8Rt1SLPBjms"
      },
      "source": [
        "fine_tuningで学習させるデータファイルを設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvm1T2ehzL8h",
        "outputId": "43888035-ceca-49a2-9a21-613f865e1d65"
      },
      "source": [
        "import os\n",
        "DIR = '/content/17flowers/train_images'\n",
        "\n",
        "print(sum(os.path.isfile(os.path.join(DIR, name)) for name in os.listdir(DIR)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}