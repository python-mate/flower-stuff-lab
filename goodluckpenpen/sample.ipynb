{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sample.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1syHXpqnvDWHR4QREJJuO16uQQoXybUYq",
      "authorship_tag": "ABX9TyP17DSv32aYrMdd+QwDJHZJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuu-eguci/flower-stuff-lab/blob/main/goodluckpenpen/sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ1a90xtqbvb"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZdLuQTAX5SQ",
        "outputId": "8161d416-64f4-4a44-8bbd-28494e673fbd"
      },
      "source": [
        "# Google Drive をマウントします。\n",
        "# NOTE: 左のトコをポチポチやってマウントすることも出来ますが、マウントすることを明示するほうが好みなのでしています。\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRLUdJXesE2Y"
      },
      "source": [
        "# 17flowers dataset を取得します。\n",
        "# NOTE: マイドライブに 17flowers.zip を置いてある必要があります。自分で用意してください。\n",
        "#       /content は一時領域なので、数時間ごとに消滅します。\n",
        "#       だからイチイチ、 unzip しています。\n",
        "!unzip \"/content/drive/MyDrive/datasets/17flowers.zip\" -d \"/content\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-rKPiPDsd3A"
      },
      "source": [
        "# 学習用画像が格納されているフォルダです。\n",
        "TRAIN_DIR = '/content/17flowers/train_images'\n",
        "# テスト用画像が格納されているフォルダです。\n",
        "TEST_DIR = '/content/17flowers/test_images'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryHE4jKyseFb"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Input\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr_zcXY0seKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ff7e878-4e8e-4709-82ad-9a7a312a0481"
      },
      "source": [
        "# VGG16 のデフォルトである 224x224 でインプットを定義します。\n",
        "# NOTE: <class 'keras.engine.keras_tensor.KerasTensor'>\n",
        "# NOTE: Tensor は\n",
        "#       > 線形的な量または線形的な幾何概念を一般化したもので、基底を選べば、多次元の配列として表現できるようなものである。\n",
        "#       > (Wikipedia より)\n",
        "#       です。(?)\n",
        "input_tensor = Input(shape=(224, 224, 3))\n",
        "\n",
        "# VGG16 をロードします。\n",
        "# が、今回はフル結合3層をつけずにロードしています。\n",
        "# NOTE: include_top=False がフル結合3層をつけないという指定です。\n",
        "#       VGG16 は畳み込み13層とフル結合3層の計16層から成ります。\n",
        "#       なので、いうなれば VGG13 になってるってこと。\n",
        "#       (これは理解しやすいからそう書いているだけで誤解なきよう。)\n",
        "# NOTE: <class 'keras.engine.functional.Functional'>\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
        "\n",
        "# 新たな層を追加しています。\n",
        "# この output っていうのが現在の最後の13層目のことです。\n",
        "# NOTE: <class 'keras.engine.keras_tensor.KerasTensor'>\n",
        "x = base_model.output\n",
        "\n",
        "# GlobalAveragePooling2D という層を足しています。(14層目)\n",
        "# NOTE: さっき VGG13 だったのが VGG14 になるってこと。\n",
        "#       (これは理解しやすいからそう書いているだけなので他所で言わないように。)\n",
        "# NOTE: Dense は時間がかかるが GlobalAveragePooling は高速だという話です。\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Dense という層を足しています。(15層目)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "\n",
        "# ここが自分の追加したい層。(16層目)\n",
        "y = Dense(18, activation='softmax')(x)\n",
        "\n",
        "# 完成したこれが層のかたまり。\n",
        "# NOTE: x とか y とかって変数名を使っているのは、このモデルの構築手順は数式で表せる(らしい)からです。\n",
        "#       こんな感じに。こういうのって数式って言うの? 方程式じゃなくて?\n",
        "#       y = Dense(Dense(GlobalAveragePooling2D(x)))\n",
        "# NOTE: <class 'keras.engine.functional.Functional'>\n",
        "model = Model(inputs=base_model.input, outputs=y)\n",
        "\n",
        "# 構築した改造 VGG16 を閲覧します。\n",
        "# VGG1 6の構造に加え、最後に層が追加されている事がわかります。\n",
        "# NOTE: こういうのが増えてる\n",
        "#       dense_1 (Dense)              (None, 17)                17425\n",
        "#       たぶん17ってのが Dense(17... で追加した層だろう。\n",
        "model.summary()\n",
        "\n",
        "# VGG16 の全層の重みを固定しています。\n",
        "# VGG16 側の層の重みは学習時に変更されません。\n",
        "# base_model は最初に用意した13層のこと。\n",
        "# これはもう学習終わってんのだから(imagenet で)、 train する必要なしです。\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# モデルを作っただけだと線形の結果しか出ません。\n",
        "# y = a * b * c * x みたいなものだからです。\n",
        "# 係数を自分で変更させるように……\n",
        "model.compile(\n",
        "    optimizer=SGD(\n",
        "        # NOTE: サンプルコードでは lr になっていたが、\n",
        "        #       UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
        "        #       が出るので learning_rate へ変更しました。\n",
        "        learning_rate=0.0001,\n",
        "        momentum=0.9,\n",
        "    ),\n",
        "    # 右辺と左辺の差を小さくするためのもの。微分です。\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# Training に使う画像を生成する ImageDataGenerator を作ります。\n",
        "# NOTE: ImageDataGenerator は与えた画像をいじり、 training に使う画像パターンを増やします。\n",
        "#       https://keras.io/ja/preprocessing/image/\n",
        "# NOTE: <class 'keras.preprocessing.image.ImageDataGenerator'>\n",
        "image_data_generator_to_train = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=10,\n",
        ")\n",
        "\n",
        "# 予測? 類推? 推測?(用語がわからん)テストに使う画像を生成する ImageDataGenerator を作ります。\n",
        "# NOTE: どうして小さくしているのかというと、モデルは小数点で学習するからです。\n",
        "#       どういうこと?\n",
        "image_data_generator_to_test = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        ")\n",
        "\n",
        "# ImageDataGenerator へ画像を与えます。\n",
        "# NOTE: <class 'keras.preprocessing.image.DirectoryIterator'>\n",
        "directory_iterator_for_training = image_data_generator_to_train.flow_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "directory_iterator_for_test = image_data_generator_to_test.flow_from_directory(\n",
        "    TEST_DIR,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# ここで学習を行います。なので時間かかります。\n",
        "# fit は学習のメソッドです。\n",
        "# NOTE: サンプルコードでは Model.fit_generator になっていたが、\n",
        "#       UserWarning: `Model.fit_generator` is deprecated and\n",
        "#                    will be removed in a future version.\n",
        "#                    Please use `Model.fit`, which supports generators.\n",
        "#       が出るため model.fit に変更しました。\n",
        "# NOTE: <class 'keras.callbacks.History'>\n",
        "history = model.fit(\n",
        "    directory_iterator_for_training,\n",
        "    # NOTE: 1260 はトレーニング用の枚数です。 70*18=1260\n",
        "    steps_per_epoch=1260 // 16,\n",
        "    epochs=40,\n",
        "    verbose=1,\n",
        "    validation_data=directory_iterator_for_test,\n",
        "    # NOTE: 180 はテスト用の枚数です。 10*18=180\n",
        "    validation_steps=180 // 16,\n",
        ")\n",
        "print(type(history))\n",
        "\n",
        "# 保存したものを予測にしか使わないなら include_optimizer=False を設定しておくと、サイズが半分以下になるそうだ。\n",
        "# NOTE: 保存したファイルを利用するのは別ファイルです。\n",
        "model.save('17flowers.hdf5')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 18)                18450     \n",
            "=================================================================\n",
            "Total params: 15,258,450\n",
            "Trainable params: 15,258,450\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Found 1260 images belonging to 18 classes.\n",
            "Found 180 images belonging to 18 classes.\n",
            "Epoch 1/60\n",
            "78/78 [==============================] - 67s 292ms/step - loss: 2.9411 - accuracy: 0.0300 - val_loss: 2.8896 - val_accuracy: 0.0682\n",
            "Epoch 2/60\n",
            "78/78 [==============================] - 20s 255ms/step - loss: 2.8801 - accuracy: 0.0672 - val_loss: 2.8635 - val_accuracy: 0.0966\n",
            "Epoch 3/60\n",
            "78/78 [==============================] - 20s 259ms/step - loss: 2.8639 - accuracy: 0.0836 - val_loss: 2.8430 - val_accuracy: 0.1250\n",
            "Epoch 4/60\n",
            "78/78 [==============================] - 20s 260ms/step - loss: 2.8407 - accuracy: 0.1378 - val_loss: 2.8250 - val_accuracy: 0.1648\n",
            "Epoch 5/60\n",
            "78/78 [==============================] - 20s 258ms/step - loss: 2.8297 - accuracy: 0.1675 - val_loss: 2.8073 - val_accuracy: 0.2159\n",
            "Epoch 6/60\n",
            "78/78 [==============================] - 20s 256ms/step - loss: 2.8058 - accuracy: 0.1975 - val_loss: 2.7927 - val_accuracy: 0.2443\n",
            "Epoch 7/60\n",
            "78/78 [==============================] - 20s 254ms/step - loss: 2.7942 - accuracy: 0.2093 - val_loss: 2.7778 - val_accuracy: 0.2784\n",
            "Epoch 8/60\n",
            "78/78 [==============================] - 20s 256ms/step - loss: 2.7818 - accuracy: 0.2608 - val_loss: 2.7634 - val_accuracy: 0.3295\n",
            "Epoch 9/60\n",
            "78/78 [==============================] - 20s 257ms/step - loss: 2.7714 - accuracy: 0.2835 - val_loss: 2.7458 - val_accuracy: 0.3295\n",
            "Epoch 10/60\n",
            "78/78 [==============================] - 20s 255ms/step - loss: 2.7509 - accuracy: 0.3085 - val_loss: 2.7328 - val_accuracy: 0.3580\n",
            "Epoch 11/60\n",
            "78/78 [==============================] - 20s 257ms/step - loss: 2.7361 - accuracy: 0.3353 - val_loss: 2.7157 - val_accuracy: 0.3807\n",
            "Epoch 12/60\n",
            "78/78 [==============================] - 20s 255ms/step - loss: 2.7307 - accuracy: 0.3199 - val_loss: 2.7048 - val_accuracy: 0.3864\n",
            "Epoch 13/60\n",
            "78/78 [==============================] - 20s 256ms/step - loss: 2.7217 - accuracy: 0.3569 - val_loss: 2.6918 - val_accuracy: 0.4148\n",
            "Epoch 14/60\n",
            "78/78 [==============================] - 20s 256ms/step - loss: 2.6976 - accuracy: 0.3812 - val_loss: 2.6804 - val_accuracy: 0.4205\n",
            "Epoch 15/60\n",
            "78/78 [==============================] - 20s 254ms/step - loss: 2.6938 - accuracy: 0.3918 - val_loss: 2.6656 - val_accuracy: 0.4659\n",
            "Epoch 16/60\n",
            "78/78 [==============================] - 20s 256ms/step - loss: 2.6788 - accuracy: 0.4207 - val_loss: 2.6559 - val_accuracy: 0.4716\n",
            "Epoch 17/60\n",
            "78/78 [==============================] - 20s 255ms/step - loss: 2.6618 - accuracy: 0.4436 - val_loss: 2.6391 - val_accuracy: 0.4602\n",
            "Epoch 18/60\n",
            "78/78 [==============================] - 20s 256ms/step - loss: 2.6595 - accuracy: 0.4182 - val_loss: 2.6288 - val_accuracy: 0.4489\n",
            "Epoch 19/60\n",
            "78/78 [==============================] - 20s 257ms/step - loss: 2.6403 - accuracy: 0.4500 - val_loss: 2.6177 - val_accuracy: 0.4545\n",
            "Epoch 20/60\n",
            "78/78 [==============================] - 20s 255ms/step - loss: 2.6274 - accuracy: 0.4861 - val_loss: 2.6059 - val_accuracy: 0.4716\n",
            "Epoch 21/60\n",
            "78/78 [==============================] - 20s 252ms/step - loss: 2.6223 - accuracy: 0.4656 - val_loss: 2.5884 - val_accuracy: 0.5114\n",
            "Epoch 22/60\n",
            "78/78 [==============================] - 20s 252ms/step - loss: 2.6059 - accuracy: 0.4645 - val_loss: 2.5807 - val_accuracy: 0.4886\n",
            "Epoch 23/60\n",
            "78/78 [==============================] - 20s 252ms/step - loss: 2.6044 - accuracy: 0.4356 - val_loss: 2.5633 - val_accuracy: 0.5341\n",
            "Epoch 24/60\n",
            "78/78 [==============================] - 20s 253ms/step - loss: 2.5949 - accuracy: 0.4797 - val_loss: 2.5536 - val_accuracy: 0.5114\n",
            "Epoch 25/60\n",
            "78/78 [==============================] - 20s 254ms/step - loss: 2.5802 - accuracy: 0.4876 - val_loss: 2.5488 - val_accuracy: 0.4602\n",
            "Epoch 26/60\n",
            "78/78 [==============================] - 20s 255ms/step - loss: 2.5640 - accuracy: 0.4893 - val_loss: 2.5349 - val_accuracy: 0.4886\n",
            "Epoch 27/60\n",
            "78/78 [==============================] - 20s 252ms/step - loss: 2.5401 - accuracy: 0.5076 - val_loss: 2.5180 - val_accuracy: 0.5284\n",
            "Epoch 28/60\n",
            "78/78 [==============================] - 20s 256ms/step - loss: 2.5353 - accuracy: 0.5116 - val_loss: 2.5142 - val_accuracy: 0.5341\n",
            "Epoch 29/60\n",
            "78/78 [==============================] - 20s 256ms/step - loss: 2.5282 - accuracy: 0.5248 - val_loss: 2.4938 - val_accuracy: 0.5284\n",
            "Epoch 30/60\n",
            "78/78 [==============================] - 20s 255ms/step - loss: 2.5165 - accuracy: 0.4897 - val_loss: 2.4812 - val_accuracy: 0.5284\n",
            "Epoch 31/60\n",
            "78/78 [==============================] - 20s 256ms/step - loss: 2.5128 - accuracy: 0.5234 - val_loss: 2.4703 - val_accuracy: 0.5000\n",
            "Epoch 32/60\n",
            "78/78 [==============================] - 20s 259ms/step - loss: 2.4971 - accuracy: 0.5340 - val_loss: 2.4579 - val_accuracy: 0.5284\n",
            "Epoch 33/60\n",
            "78/78 [==============================] - 20s 255ms/step - loss: 2.4781 - accuracy: 0.5412 - val_loss: 2.4506 - val_accuracy: 0.5398\n",
            "Epoch 34/60\n",
            "78/78 [==============================] - 20s 258ms/step - loss: 2.4655 - accuracy: 0.5470 - val_loss: 2.4385 - val_accuracy: 0.5398\n",
            "Epoch 35/60\n",
            "78/78 [==============================] - 20s 258ms/step - loss: 2.4537 - accuracy: 0.5327 - val_loss: 2.4281 - val_accuracy: 0.5625\n",
            "Epoch 36/60\n",
            "78/78 [==============================] - 20s 259ms/step - loss: 2.4539 - accuracy: 0.5603 - val_loss: 2.4151 - val_accuracy: 0.5341\n",
            "Epoch 37/60\n",
            "78/78 [==============================] - 20s 260ms/step - loss: 2.4330 - accuracy: 0.5604 - val_loss: 2.4001 - val_accuracy: 0.5739\n",
            "Epoch 38/60\n",
            "78/78 [==============================] - 20s 258ms/step - loss: 2.4104 - accuracy: 0.5507 - val_loss: 2.3877 - val_accuracy: 0.5795\n",
            "Epoch 39/60\n",
            "78/78 [==============================] - 20s 255ms/step - loss: 2.4235 - accuracy: 0.5500 - val_loss: 2.3718 - val_accuracy: 0.5682\n",
            "Epoch 40/60\n",
            "78/78 [==============================] - 20s 256ms/step - loss: 2.4212 - accuracy: 0.5341 - val_loss: 2.3640 - val_accuracy: 0.5625\n",
            "Epoch 41/60\n",
            "78/78 [==============================] - 20s 257ms/step - loss: 2.3929 - accuracy: 0.5531 - val_loss: 2.3507 - val_accuracy: 0.5625\n",
            "Epoch 42/60\n",
            "78/78 [==============================] - 20s 257ms/step - loss: 2.3833 - accuracy: 0.5429 - val_loss: 2.3411 - val_accuracy: 0.5795\n",
            "Epoch 43/60\n",
            "78/78 [==============================] - 20s 258ms/step - loss: 2.3708 - accuracy: 0.5517 - val_loss: 2.3305 - val_accuracy: 0.5966\n",
            "Epoch 44/60\n",
            "78/78 [==============================] - 20s 258ms/step - loss: 2.3703 - accuracy: 0.5793 - val_loss: 2.3138 - val_accuracy: 0.5739\n",
            "Epoch 45/60\n",
            "78/78 [==============================] - 20s 257ms/step - loss: 2.3481 - accuracy: 0.5594 - val_loss: 2.3131 - val_accuracy: 0.5625\n",
            "Epoch 46/60\n",
            "78/78 [==============================] - 20s 256ms/step - loss: 2.3475 - accuracy: 0.5740 - val_loss: 2.2988 - val_accuracy: 0.5682\n",
            "Epoch 47/60\n",
            "78/78 [==============================] - 20s 259ms/step - loss: 2.3263 - accuracy: 0.5633 - val_loss: 2.2863 - val_accuracy: 0.5852\n",
            "Epoch 48/60\n",
            "78/78 [==============================] - 20s 256ms/step - loss: 2.3250 - accuracy: 0.5605 - val_loss: 2.2733 - val_accuracy: 0.5739\n",
            "Epoch 49/60\n",
            "78/78 [==============================] - 20s 257ms/step - loss: 2.3092 - accuracy: 0.5782 - val_loss: 2.2626 - val_accuracy: 0.5682\n",
            "Epoch 50/60\n",
            "78/78 [==============================] - 20s 259ms/step - loss: 2.2685 - accuracy: 0.6055 - val_loss: 2.2582 - val_accuracy: 0.5966\n",
            "Epoch 51/60\n",
            "78/78 [==============================] - 20s 256ms/step - loss: 2.2810 - accuracy: 0.5624 - val_loss: 2.2383 - val_accuracy: 0.5852\n",
            "Epoch 52/60\n",
            "78/78 [==============================] - 20s 257ms/step - loss: 2.2789 - accuracy: 0.5947 - val_loss: 2.2193 - val_accuracy: 0.6023\n",
            "Epoch 53/60\n",
            "78/78 [==============================] - 20s 261ms/step - loss: 2.2582 - accuracy: 0.6105 - val_loss: 2.2148 - val_accuracy: 0.5909\n",
            "Epoch 54/60\n",
            "78/78 [==============================] - 20s 261ms/step - loss: 2.2389 - accuracy: 0.6128 - val_loss: 2.2000 - val_accuracy: 0.6080\n",
            "Epoch 55/60\n",
            "78/78 [==============================] - 20s 258ms/step - loss: 2.2508 - accuracy: 0.5940 - val_loss: 2.1885 - val_accuracy: 0.6364\n",
            "Epoch 56/60\n",
            "78/78 [==============================] - 20s 258ms/step - loss: 2.2339 - accuracy: 0.6011 - val_loss: 2.1788 - val_accuracy: 0.6364\n",
            "Epoch 57/60\n",
            "78/78 [==============================] - 20s 257ms/step - loss: 2.2281 - accuracy: 0.6247 - val_loss: 2.1759 - val_accuracy: 0.5966\n",
            "Epoch 58/60\n",
            "78/78 [==============================] - 20s 258ms/step - loss: 2.2252 - accuracy: 0.5954 - val_loss: 2.1620 - val_accuracy: 0.6250\n",
            "Epoch 59/60\n",
            "78/78 [==============================] - 20s 257ms/step - loss: 2.1907 - accuracy: 0.5924 - val_loss: 2.1488 - val_accuracy: 0.6193\n",
            "Epoch 60/60\n",
            "78/78 [==============================] - 20s 258ms/step - loss: 2.1909 - accuracy: 0.6056 - val_loss: 2.1409 - val_accuracy: 0.6250\n",
            "<class 'keras.callbacks.History'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0TlommCseQT"
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, GlobalAveragePooling2D, Dense\n",
        "from keras.preprocessing import image\n",
        "import numpy"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyuXOlp0seXA"
      },
      "source": [
        "CLASSES_FOR_17FLOWERS = [\n",
        "    'Tulip', 'Snowdrop', 'LilyValley', 'Bluebell', 'Crocus',\n",
        "    'Iris', 'Tigerlily', 'Daffodil', 'Fritillary', 'Sunflower',\n",
        "    'Daisy', 'ColtsFoot', 'Dandelion', 'Cowslip', 'Buttercup',\n",
        "    'Windflower', 'Pansy', 'Anthurium',\n",
        "]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7Ypr9bbseZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b57bb14-7896-46bf-89a5-7f0bc8aaa944"
      },
      "source": [
        "# test_vgg16.py で行っているのと同じように、\n",
        "# 引数で与えられた画像を、予測へまわせる形式へ変換します。\n",
        "# <class 'PIL.Image.Image'>\n",
        "#       -> 3次元テンソル <class 'numpy.ndarray'>\n",
        "#       -> 4次元テンソル <class 'numpy.ndarray'>\n",
        "image_pil = image.load_img('/content/drive/MyDrive/keras-vgg16-lab/test-images/sunflower.jpg', target_size=(224, 224))\n",
        "image_array_3dim = image.img_to_array(image_pil)\n",
        "image_array_4dim = numpy.expand_dims(image_array_3dim, axis=0)\n",
        "\n",
        "# 学習時(fine_tuning_17flowers.py)において、 ImageDataGenerator の rescale で正規化したので同じ処理が必要。\n",
        "# XXX: ……らしいですよくわかんないです。\n",
        "# これを忘れると結果がおかしくなる。\n",
        "# XXX: ……らしいですよくわかんないです。\n",
        "# NOTE: どうおかしくなるのかわからなかったのでこれをやらないで試しました。\n",
        "#       やって sunflower.jpg 試した場合↓\n",
        "#           ('Cowslip', 0.14450616)\n",
        "#           ('Sunflower', 0.08813832)\n",
        "#           ('Tigerlily', 0.07421722)\n",
        "#       やらないで試した場合↓\n",
        "#           ('Cowslip', 0.9992175)\n",
        "#           ('Tigerlily', 0.0007754794)\n",
        "#           ('Buttercup', 6.109471e-06)\n",
        "image_array_4dim = image_array_4dim / 255.0\n",
        "\n",
        "# fine_tuning_17flowers.py で一番最初に作っているのと同じ、 VGG13(仮) + 自作3層 を用意します。\n",
        "# NOTE: Fine tuning 関連の記事ではよく Sequential が使われているけどまずこれでいく。\n",
        "input_tensor = Input(shape=(224, 224, 3))\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "y = Dense(18, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=y)\n",
        "\n",
        "# Fine tuning ではこのあと重み固定(layer.trainable = False)とかをしています。\n",
        "# 今回は学習を行う必要がありません。 Weight file はすでにあるので。\n",
        "model.load_weights('/content/17flowers.hdf5')\n",
        "\n",
        "# NOTE: predict 前に model.compile する必要はありますか?\n",
        "#       試しました。\n",
        "#       やって sunflower.jpg 試した場合↓\n",
        "#           ('Cowslip', 0.14450616)\n",
        "#           ('Sunflower', 0.08813832)\n",
        "#           ('Tigerlily', 0.07421722)\n",
        "#       やらないで試した場合。あ、変わらないじゃん。やらないでいいみたい。\n",
        "#           ('Cowslip', 0.14450616)\n",
        "#           ('Sunflower', 0.08813832)\n",
        "#           ('Tigerlily', 0.07421722)\n",
        "\n",
        "# 予測を行います。\n",
        "# NOTE: test_vgg16 では\n",
        "#       predictions = vgg16.predict(preprocess_input(image_array_4dim))\n",
        "#       top_5_predictions = decode_predictions(predictions, top=5)[0]\n",
        "#       でしたね。どういうことやねん。\n",
        "# NOTE: test_vgg16.py では preprocess_input を使っていたのにどうしてこっちでは使わないのだろう?\n",
        "#       と思ったので使って試しました。\n",
        "#       使わず sunflower.jpg 試した場合↓\n",
        "#           ('Cowslip', 0.14450616)\n",
        "#           ('Sunflower', 0.08813832)\n",
        "#           ('Tigerlily', 0.07421722)\n",
        "#       使って試した場合。 Sunflower が消えた。\n",
        "#           ('Iris', 0.16075435)\n",
        "#           ('Tulip', 0.15425675)\n",
        "#           ('Fritillary', 0.13122706)\n",
        "prediction = model.predict(image_array_4dim)[0]\n",
        "top_indices = prediction.argsort()[-5:][::-1]\n",
        "result = [(CLASSES_FOR_17FLOWERS[i], prediction[i]) for i in top_indices]\n",
        "for x in result:\n",
        "    print(x)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Buttercup', 0.15318406)\n",
            "('Daffodil', 0.086284235)\n",
            "('Anthurium', 0.07820909)\n",
            "('Daisy', 0.07391712)\n",
            "('LilyValley', 0.061176542)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XOXfkjXsebV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8Rt1SLPBjms"
      },
      "source": [
        "fine_tuningで学習させるデータファイルを設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvm1T2ehzL8h",
        "outputId": "43888035-ceca-49a2-9a21-613f865e1d65"
      },
      "source": [
        "import os\n",
        "DIR = '/content/17flowers/train_images'\n",
        "\n",
        "print(sum(os.path.isfile(os.path.join(DIR, name)) for name in os.listdir(DIR)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}